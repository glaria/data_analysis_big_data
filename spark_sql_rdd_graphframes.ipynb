{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glaria/data_analysis_big_data/blob/main/spark_sql_rdd_graphframes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH9_zbDWpNSw"
      },
      "source": [
        "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
        "# PEC 4\n",
        "\n",
        "Ex. created by: Joan Tomas Matamalas Llodra jtmatamalas@uoc.edu\n",
        "\n",
        "## Knowledge extraction from heterogeneous data sources using Spark SQL, RDDs and GraphFrames\n",
        "\n",
        "- **Part 0:** Environment setup\n",
        "- **Part 1:** Introduction to structured data frames and how to operate extract information \n",
        "    - **Part 1.1:** Import the data \n",
        "    - **Part 1.2:** *Queries* about complex data frames \n",
        "        - **Part 1.2.1:** SQL Queries \n",
        "        - **Part 1.2.2:** Queries about the pipeline \n",
        "- **Part 2:** HIVE databases and complex operations \n",
        "    - **Part 2.1:** Hive databases\n",
        "    - **Part 2.2:** Beyond SQL Transformations \n",
        "        - **Part 2.2.1:** Tweets per population \n",
        "            - **Part 2.2.1.1:** Using SQL \n",
        "            - **Part 2.2.1.2:** Using RDD \n",
        "        - **Part 2.2.2:** Count hashtags \n",
        "- **Part 3:** Sampling \n",
        "    - **Part 3.1:** Homogeneous \n",
        "    - **Part 3.2:** Stratified \n",
        "- **Part 4**: Introduction to relational data \n",
        "    - **Part 4.1:** Generate the retweet network \n",
        "        - **Part 4.1.1**: Construction of the edgelist \n",
        "        - **Part 4.1.2**: Degree centrality \n",
        "    - **Part 4.2:** Network analysis using GraphFrames \n",
        "        - **Part 4.2.1:** Create a graph frame \n",
        "        - **Part 4.2.2:** PageRank Centrality "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scrMoVU1pNS0"
      },
      "source": [
        "## **Part 0:** Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lq-quPDpNS1"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J7p0JBxpNS2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from math import floor\n",
        "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
        "from pyspark.sql import Row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuDziguTpNS2"
      },
      "outputs": [],
      "source": [
        "SUBMIT_ARGS = \"--packages graphframes:graphframes:0.7.0-spark2.4-s_2.11 pyspark-shell\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
        "\n",
        "conf = SparkConf()\n",
        "conf.setMaster(\"local[1]\")\n",
        "# Introducid el nombre de la app PEC3_ seguido de vuestro nombre de usuario\n",
        "conf.setAppName(\"PAC3_lglaria\")\n",
        "sc = SparkContext(conf=conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "4XlT2-TxpNS3"
      },
      "source": [
        "## **Part 1:** Introduction to structured data frames and operations on them.\n",
        "\n",
        "As already mentioned, in this practice we are going to use Twitter data that we collected during the general elections in Spain on April 28, 2019. As we will see, the tweets have a rather complex internal structure that we have simplified a bit in this practice.\n",
        "\n",
        "### **Part 1.1:** Import the data\n",
        "\n",
        "The first thing we are going to learn is how to import this type of data into our environment. One of the most common types of files to store this information format is [the JSON structure] (https://en.wikipedia.org/wiki/JSON). This structure allows information to be saved in a plain text of different objects following a dictionary structure where each field is assigned a key and a value. The structure can be nested, that is, a key can have another dictionary-type structure as its value.\n",
        "\n",
        "Spark SQL allows you to read data from many different formats (as you will remember from the previous practice where we read a CSV file). This time, you are asked to read a JSON file from the path ```/aula_M2.858/data/tweets28a_sample.json```. This file contains a small *sample*, 0.1% of the complete database (in a next section we will see how to perform this *sampling*). This time you are not asked to specify the structure of the data frame since the read function will infer it automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5djXyi2ypNS3",
        "outputId": "30cbe0f3-869f-49d7-9bf1-808acfec36e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset contains 27268 tweets\n"
          ]
        }
      ],
      "source": [
        "sqlContext = SQLContext(sc)\n",
        "tweets_sample = sqlContext.read.json(\"/aula_M2.858/data/tweets28a_sample.json\")\n",
        "\n",
        "print(\"Loaded dataset contains %d tweets\" % tweets_sample.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK8K69rKpNS4"
      },
      "source": [
        "The next step is to show the structure of the dataset that we just loaded. \n",
        "\n",
        "---\n",
        "\n",
        "Remember that you can get the information about how the DataTable is structured using the method ``` printSchema ()```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmbx2L63pNS5",
        "outputId": "2b78a54c-b883-417d-ef13-a8edd42233ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _id: string (nullable = true)\n",
            " |-- created_at: long (nullable = true)\n",
            " |-- lang: string (nullable = true)\n",
            " |-- place: struct (nullable = true)\n",
            " |    |-- bounding_box: struct (nullable = true)\n",
            " |    |    |-- coordinates: array (nullable = true)\n",
            " |    |    |    |-- element: array (containsNull = true)\n",
            " |    |    |    |    |-- element: array (containsNull = true)\n",
            " |    |    |    |    |    |-- element: double (containsNull = true)\n",
            " |    |    |-- type: string (nullable = true)\n",
            " |    |-- country_code: string (nullable = true)\n",
            " |    |-- id: string (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- place_type: string (nullable = true)\n",
            " |-- retweeted_status: struct (nullable = true)\n",
            " |    |-- _id: string (nullable = true)\n",
            " |    |-- user: struct (nullable = true)\n",
            " |    |    |-- followers_count: long (nullable = true)\n",
            " |    |    |-- friends_count: long (nullable = true)\n",
            " |    |    |-- id_str: string (nullable = true)\n",
            " |    |    |-- lang: string (nullable = true)\n",
            " |    |    |-- screen_name: string (nullable = true)\n",
            " |    |    |-- statuses_count: long (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- user: struct (nullable = true)\n",
            " |    |-- followers_count: long (nullable = true)\n",
            " |    |-- friends_count: long (nullable = true)\n",
            " |    |-- id_str: string (nullable = true)\n",
            " |    |-- lang: string (nullable = true)\n",
            " |    |-- screen_name: string (nullable = true)\n",
            " |    |-- statuses_count: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tweets_sample.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8rWc_f6pNS6"
      },
      "source": [
        "You can see that the tweet structure contains multiple nested fields. You have to familiarize yourself with this structure since it will be the one that we will use throughout the practice. Remember also that not all tweets have all fields, such as location (field ```place```). When this happens the field becomes ```NULL```. You can see more information about this type of data in [this link] (https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "y2REWQW4pNS6"
      },
      "source": [
        "### **Part 1.2:** *Queries* about complex data frames\n",
        "\n",
        "In the previous practice we have seen how to query a very simple dataset using *SQL* statements. In this part we are going to refresh the concepts used in the previous practice by introducing some more advanced concepts and a new way of working on data tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORUInbMwpNS6"
      },
      "source": [
        "#### **Part 1.2.1:** SQL Queries\n",
        "\n",
        "As you will remember from part 3 of the previous PEC, the first step is to register the table in the SQL context, first checking if it exists and deleting it if it does. In this section you are asked to register the table ```tweets_sample``` that we just loaded in the sql context under the same name``` tweets_sample```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3lXdzZipNS7"
      },
      "outputs": [],
      "source": [
        "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_sample\")\n",
        "sqlContext.registerDataFrameAsTable(tweets_sample,\"tweets_sample\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpFMW2Q-pNS7"
      },
      "source": [
        "Now you are asked to create a table ```users_agg``` with [the added information] (https://www.w3schools.com/sql/sql_groupby.asp) of the users who have their language defined (``` user.lang```) as Spanish (```es```). Specifically, you are asked that the table contain the following columns:\n",
        "- **screen_name:** username\n",
        "- **friends_count:** maximum number (see note) of people you follow\n",
        "- **tweets:** number of tweets made\n",
        "- **followers_count:** maximum number (see note) people who follow the user.\n",
        "\n",
        "The order in which the records should be displayed is descending order according to the number of tweets.\n",
        "\n",
        "***Note:*** It is important that you pay attention that the name of *friends* and *followers* may differ throughout the data acquisition. In this case we are going to use the aggregation function ```MAX``` on each of these fields to avoid segmenting the user in different instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpE7r4WXpNS8",
        "outputId": "099a2c8c-52f6-45b3-b485-60236512bd89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+------+-------------+---------------+\n",
            "|    screen_name|tweets|friends_count|followers_count|\n",
            "+---------------+------+-------------+---------------+\n",
            "|       anaoromi|    16|         6258|           6774|\n",
            "|    RosaMar6254|    14|         6208|           6245|\n",
            "|        lyuva26|    13|         3088|           3732|\n",
            "|PisandoFuerte10|    12|         2795|           1752|\n",
            "|     carrasquem|    12|          147|            215|\n",
            "|       jasalo54|    11|         1889|            689|\n",
            "|  PabloChabolas|     9|         4925|           4042|\n",
            "|      lolalailo|     9|         4922|           3738|\n",
            "|     Lordcrow11|     9|         5002|           3069|\n",
            "|    DuroBelinda|     9|         5242|           5778|\n",
            "| locuspolitikus|     9|        11261|          10244|\n",
            "|      kikyosanz|     9|          154|            273|\n",
            "|  Rafa_eltorete|     9|          908|           1060|\n",
            "|   rosavergar23|     8|          900|           1224|\n",
            "|        Fermirv|     8|         3031|           1731|\n",
            "|       LianaEHE|     8|         5272|           5014|\n",
            "|    SSarelvis67|     8|         3287|           2323|\n",
            "| BlaancaNiieves|     8|         7023|          12630|\n",
            "|       anap1958|     8|          132|            400|\n",
            "|   merchi_otero|     8|          701|            852|\n",
            "+---------------+------+-------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "users_agg = sqlContext.sql(\"SELECT user.screen_name,COUNT(*) as tweets, MAX(user.friends_count)as friends_count, MAX(user.followers_count) as followers_count FROM tweets_sample WHERE user.lang = 'es' GROUP BY  user.screen_name ORDER BY COUNT(*) DESC\")\n",
        "users_agg.limit(100).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-0nxSerpNS8"
      },
      "outputs": [],
      "source": [
        "output = users_agg.first()\n",
        "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy1P9_R4pNS8"
      },
      "source": [
        "Now imagine that we want to combine the information we just generated with information about the number of times a user has been retweeted. To do this type of combination we need to resort to the [```JOIN``` of tables] (https://www.w3schools.com/sql/sql_join.asp). We must first register the table we just generated in the SQL context. Remember that you must first check if the table exists and if so, delete it. You have to register the table under the name of ```user_agg```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1pqIZu7pNS8"
      },
      "outputs": [],
      "source": [
        "sqlContext.sql(\"DROP TABLE IF EXISTS users_agg\")\n",
        "sqlContext.registerDataFrameAsTable(users_agg,\"users_agg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7VuBUnwpNS9"
      },
      "source": [
        "Once registered, you are asked to combine this table and the table ```tweets_sample``` using an ```INNER JOIN``` to obtain a new table with the following information:\n",
        "- ***screen_name:*** username\n",
        "- ***friends_count:*** maximum number of people you follow\n",
        "- ***followers_count:*** maximum number of people who follow the user.\n",
        "- ***tweets:*** number of tweets made by the user.\n",
        "- ***retweeted:*** number of retweets obtained by the user.\n",
        "- ***ratio_tweet_retweeted:*** ratio of retweets by number of tweets published $\\frac {retweets} {tweets} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo5rBiCxpNS9",
        "outputId": "622abdb4-f168-49fc-811d-f06a11b72644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-------------+---------------+------+---------+---------------------+\n",
            "|   screen_name|friends_count|followers_count|tweets|retweeted|ratio_tweet_retweeted|\n",
            "+--------------+-------------+---------------+------+---------+---------------------+\n",
            "|          PSOE|        13635|         671073|     1|      155|                155.0|\n",
            "|  CiudadanosCs|        92910|         511896|     1|      117|                117.0|\n",
            "|     JuntsXCat|          202|          88515|     1|       73|                 73.0|\n",
            "|  PartidoPACMA|         1498|         232932|     1|       63|                 63.0|\n",
            "|  pablocasado_|         4567|         238926|     1|       50|                 50.0|\n",
            "|voxnoticias_es|         2146|          29582|     1|       44|                 44.0|\n",
            "|RaiLopezCalvet|         7579|          13574|     1|       43|                 43.0|\n",
            "|        iunida|        10225|         558318|     1|       39|                 39.0|\n",
            "|        Xuxipc|          311|         184967|     1|       37|                 37.0|\n",
            "|       Panik81|         1587|          15374|     1|       29|                 29.0|\n",
            "+--------------+-------------+---------------+------+---------+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "retweeted = sqlContext.sql(\"\"\" SELECT a.screen_name\n",
        "                                , a.friends_count\n",
        "                                , a.followers_count\n",
        "                                , a.tweets\n",
        "                                , COUNT(b.retweeted_status) as retweeted\n",
        "                                , COUNT(b.retweeted_status)/a.tweets as ratio_tweet_retweeted\n",
        "                               FROM users_agg a INNER JOIN tweets_sample b on a.screen_name = b.retweeted_status.user.screen_name\n",
        "                               GROUP BY a.screen_name, a.friends_count\n",
        "                                , a.followers_count\n",
        "                                , a.tweets\n",
        "                                ORDER BY COUNT(b.retweeted_status) DESC\n",
        "                            \"\"\")\n",
        "                           \n",
        "\n",
        "retweeted.limit(10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8am4LnApNS9"
      },
      "outputs": [],
      "source": [
        "output = retweeted.first()\n",
        "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM2lWiFGpNS9"
      },
      "source": [
        "#### **Part 1.2.2:** Queries through the pipeline\n",
        "\n",
        "Spark SQL tables provide another mechanism to apply transforms and obtain results similar to what you would get by applying an SQL query. For example, using the following pipeline we will obtain the text of all the tweets in Spanish:\n",
        "\n",
        "```\n",
        "tweets_sample.where (\"lang == 'es'\"). select (\"text\")\n",
        "```\n",
        "\n",
        "Which is equivalent to the following SQL statement:\n",
        "\n",
        "```\n",
        "SELECT text\n",
        "FROM tweets_sample\n",
        "WHERE lang == 'is'\n",
        "```\n",
        "\n",
        "You can consult the [Spark SQL API] (https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) to find more information on how to use the different transformations in tables.\n",
        "\n",
        "In this exercise you are asked to replicate the query obtained in the previous section, starting by generating the table ```users_agg```. You can use the transformations ```where```,``` select``` (or ```selectExpr```),``` groupBy```, ```count```,```agg ```and ```orderBy```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri7oIGsupNS9",
        "outputId": "88f973f7-3a90-4d03-c1e9-447f2dd57692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+--------+------------------+--------------------+\n",
            "|    screen_name|count(1)|max(friends_count)|max(followers_count)|\n",
            "+---------------+--------+------------------+--------------------+\n",
            "|       anaoromi|      16|              6258|                6774|\n",
            "|    RosaMar6254|      14|              6208|                6245|\n",
            "|        lyuva26|      13|              3088|                3732|\n",
            "|PisandoFuerte10|      12|              2795|                1752|\n",
            "|     carrasquem|      12|               147|                 215|\n",
            "|       jasalo54|      11|              1889|                 689|\n",
            "|  PabloChabolas|       9|              4925|                4042|\n",
            "|      lolalailo|       9|              4922|                3738|\n",
            "|     Lordcrow11|       9|              5002|                3069|\n",
            "|    DuroBelinda|       9|              5242|                5778|\n",
            "+---------------+--------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark.sql.functions as f\n",
        "users = tweets_sample.where( \"user.lang = 'es'\").select(\"user.screen_name\",\"user.friends_count\", \"user.followers_count\")\n",
        "users_agg = users.groupBy(\"screen_name\")\\\n",
        "                 .agg(f.count('*'), f.max(\"friends_count\"), f.max(\"followers_count\"))\\\n",
        "                 .orderBy(\"count(1)\", ascending=0)\n",
        "\n",
        "users_agg.limit(10).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1jpUUmtpNS-"
      },
      "source": [
        "If you look you will see that the name of the columns does not correspond to the one obtained previously, you can change the name of a specific column using the transformation ```withColumnRenamed```. Rename the columns to match the previous section and save them in a ```user_agg_new``` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ochOuZbpNS-",
        "outputId": "8f837f3b-c5b8-46ee-a9ac-4fd1af8627c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+------+-------------+---------------+\n",
            "|    screen_name|tweets|friends_count|followers_count|\n",
            "+---------------+------+-------------+---------------+\n",
            "|       anaoromi|    16|         6258|           6774|\n",
            "|    RosaMar6254|    14|         6208|           6245|\n",
            "|        lyuva26|    13|         3088|           3732|\n",
            "|PisandoFuerte10|    12|         2795|           1752|\n",
            "|     carrasquem|    12|          147|            215|\n",
            "|       jasalo54|    11|         1889|            689|\n",
            "|  PabloChabolas|     9|         4925|           4042|\n",
            "|      lolalailo|     9|         4922|           3738|\n",
            "|     Lordcrow11|     9|         5002|           3069|\n",
            "|    DuroBelinda|     9|         5242|           5778|\n",
            "+---------------+------+-------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "users_agg_new = users_agg.withColumnRenamed(\"count(1)\", \"tweets\")\\\n",
        "                         .withColumnRenamed(\"max(friends_count)\",\"friends_count\")\\\n",
        "                         .withColumnRenamed(\"max(followers_count)\",\"followers_count\")\n",
        "\n",
        "users_agg_new.limit(10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvqqa_LPpNS-"
      },
      "outputs": [],
      "source": [
        "output = users_agg_new.first()\n",
        "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFnndHTWpNS-"
      },
      "source": [
        "\n",
        "Now create a table ```user_retweets``` using transformations that contains two columns:\n",
        "- ***screen_name:*** username\n",
        "- ***retweeted:*** number of retweets\n",
        "\n",
        "You can use the same transformations as in the previous exercise. Sort the table in descending order using the value of the column ```retweeted```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki0mO_m7pNS_",
        "outputId": "e299a40b-af6d-4b93-fb98-e442d763323a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+---------+\n",
            "|   screen_name|retweeted|\n",
            "+--------------+---------+\n",
            "|        vox_es|      299|\n",
            "|  ahorapodemos|      238|\n",
            "| Santi_ABASCAL|      238|\n",
            "|      iescolar|      166|\n",
            "| AlbanoDante76|      161|\n",
            "|          PSOE|      155|\n",
            "|AntonioMaestre|      154|\n",
            "|          KRLS|      149|\n",
            "|        boye_g|      142|\n",
            "|  CiudadanosCs|      117|\n",
            "+--------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_retweets = tweets_sample.where(\"retweeted_status.user.statuses_count > '0'\").select(\"retweeted_status.user.screen_name\")\n",
        "user_retweets = user_retweets.groupBy(\"screen_name\")\\\n",
        "                 .agg(f.count('*'))\\\n",
        "                 .orderBy(\"count(1)\", ascending=0)\n",
        "user_retweets = user_retweets.withColumnRenamed(\"count(1)\", \"retweeted\")\n",
        "user_retweets.limit(10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSXdPZiQpNS_"
      },
      "outputs": [],
      "source": [
        "output = user_retweets.first()\n",
        "assert output.screen_name == 'vox_es' and output.retweeted == 299, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0y4atvZpNS_"
      },
      "source": [
        "Another way to join two tables is by using the [table method join] (https://spark.apache.org/docs/latest/api/python/pyspark.sql.html). Combine the information from table users_agg_new and user_retweets into a new table retweeted using columnscreen_name. Sort the new table in descending order with the name of retweets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG7qQOdlpNS_",
        "outputId": "1b78c08b-f13e-44c7-f123-b18f87fec4c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------+-------------+---------------+--------------+---------+\n",
            "|   screen_name|tweets|friends_count|followers_count|   screen_name|retweeted|\n",
            "+--------------+------+-------------+---------------+--------------+---------+\n",
            "|          PSOE|     1|        13635|         671073|          PSOE|      155|\n",
            "|  CiudadanosCs|     1|        92910|         511896|  CiudadanosCs|      117|\n",
            "|     JuntsXCat|     1|          202|          88515|     JuntsXCat|       73|\n",
            "|  PartidoPACMA|     1|         1498|         232932|  PartidoPACMA|       63|\n",
            "|  pablocasado_|     1|         4567|         238926|  pablocasado_|       50|\n",
            "|voxnoticias_es|     1|         2146|          29582|voxnoticias_es|       44|\n",
            "|RaiLopezCalvet|     1|         7579|          13574|RaiLopezCalvet|       43|\n",
            "|        iunida|     1|        10225|         558318|        iunida|       39|\n",
            "|        Xuxipc|     1|          311|         184967|        Xuxipc|       37|\n",
            "|       Panik81|     1|         1587|          15374|       Panik81|       29|\n",
            "+--------------+------+-------------+---------------+--------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "retweeted = users_agg_new.join(user_retweets, users_agg_new.screen_name == user_retweets.screen_name)\\\n",
        "                         .orderBy(user_retweets.retweeted, ascending=0)\n",
        "\n",
        "retweeted.limit(10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MakEJFPzpNS_"
      },
      "outputs": [],
      "source": [
        "output = retweeted.first()\n",
        "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.retweeted == 155, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOkMmJ0QpNS_"
      },
      "source": [
        "You will notice that some of the records that appear in the ```users_retweeted``` table are not present in the retweeted table. This is because, by default, the method applies an inner join and therefore only combines the records present in both tables. You can change this behavior through the function parameters.\n",
        "\n",
        "To finish this part and reconstruct the result of section 1.2.1 we are going to add a column ```ratio_tweet_retweeted``` with information on the ratio between retweets and tweets. For this you must use the transformation ```withColumn```. The result must be sorted considering this new column in descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVB9G_MzpNTA",
        "outputId": "33be39c5-4472-4f60-e766-3c05673d6a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------+-------------+---------------+--------------+---------+---------------------+\n",
            "|   screen_name|tweets|friends_count|followers_count|   screen_name|retweeted|ratio_tweet_retweeted|\n",
            "+--------------+------+-------------+---------------+--------------+---------+---------------------+\n",
            "|          PSOE|     1|        13635|         671073|          PSOE|      155|                155.0|\n",
            "|  CiudadanosCs|     1|        92910|         511896|  CiudadanosCs|      117|                117.0|\n",
            "|     JuntsXCat|     1|          202|          88515|     JuntsXCat|       73|                 73.0|\n",
            "|  PartidoPACMA|     1|         1498|         232932|  PartidoPACMA|       63|                 63.0|\n",
            "|  pablocasado_|     1|         4567|         238926|  pablocasado_|       50|                 50.0|\n",
            "|voxnoticias_es|     1|         2146|          29582|voxnoticias_es|       44|                 44.0|\n",
            "|RaiLopezCalvet|     1|         7579|          13574|RaiLopezCalvet|       43|                 43.0|\n",
            "|        iunida|     1|        10225|         558318|        iunida|       39|                 39.0|\n",
            "|        Xuxipc|     1|          311|         184967|        Xuxipc|       37|                 37.0|\n",
            "|       Panik81|     1|         1587|          15374|       Panik81|       29|                 29.0|\n",
            "+--------------+------+-------------+---------------+--------------+---------+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "retweeted = retweeted.withColumn('ratio_tweet_retweeted',retweeted.retweeted/retweeted.tweets)\n",
        "retweeted = retweeted.orderBy(retweeted.ratio_tweet_retweeted, ascending=0)\n",
        "retweeted.limit(10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-iCcGrwpNTA"
      },
      "outputs": [],
      "source": [
        "output = retweeted.first()\n",
        "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "zlnLD2GQpNTA"
      },
      "source": [
        "## **Part 2:** HIVE databases and complex operations\n",
        "\n",
        "So far we have been working with a small sample of the generated tweets (0.1%). In this part of the PEC we are going to see how to work and deal with the complete dataset. For this we have to use both transformations on tables and operations on RDD when necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmKhjfNGpNTA"
      },
      "source": [
        "### **Part 2.1:** Hive databases\n",
        "\n",
        "Many times the data with which we are going to work will be used in various projects. One way to organize the data is, instead of using the files directly, to use a database to manage the information. In the Hadoop environment, one of the most used databases is [Apache Hive] (https://hive.apache.org/), a database that allows you to work with distributed content.\n",
        "\n",
        "The way to access this database is by creating a Hive context in much the same way as we declare an SQL context. First of all we are going to declare a variable ```hiveContext``` by instantiating it as an object of class ```HiveContext```. Then we will check how many tables are registered in this context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfr9qr4WpNTA",
        "outputId": "bc063f79-37b5-4ffd-88fd-99e9c43352fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-------------+-----------+\n",
            "|database|    tableName|isTemporary|\n",
            "+--------+-------------+-----------+\n",
            "| default| province_28a|      false|\n",
            "| default|    tweets28a|      false|\n",
            "| default|    user_info|      false|\n",
            "| default|user_info_old|      false|\n",
            "|        |tweets_sample|       true|\n",
            "|        |    users_agg|       true|\n",
            "+--------+-------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hiveContext = HiveContext(sc)\n",
        "hiveContext.tables().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZUOFtZapNTB"
      },
      "source": [
        "Note that right now we have five tables recorded in this context. Three of them are not temporary and two are temporary, which we have previously registered. Therefore sqlContext and hiveContext are connected (it is the same session)\n",
        "\n",
        "We are going to create a variable ```tweets``` that we will use to access the table ```tweets28a``` stored in ```hiveContext``` using the method ```table ()``` of This object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXZVOmwfpNTB",
        "outputId": "528ce507-619e-4c58-a217-c8f17d3839b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset contains 25419835 tweets\n"
          ]
        }
      ],
      "source": [
        "tweets = hiveContext.table('tweets28a')\n",
        "print(\"Loaded dataset contains {} tweets\".format(tweets.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWUz_Rh5pNTB"
      },
      "source": [
        "Using the same method as in section 1.1, check the structure of the table we just loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oULb_3kKpNTB",
        "outputId": "232cd55e-013c-41e0-cbdd-a5b6bbb18ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _id: string (nullable = true)\n",
            " |-- created_at: timestamp (nullable = true)\n",
            " |-- lang: string (nullable = true)\n",
            " |-- place: struct (nullable = true)\n",
            " |    |-- bounding_box: struct (nullable = true)\n",
            " |    |    |-- coordinates: array (nullable = true)\n",
            " |    |    |    |-- element: array (containsNull = true)\n",
            " |    |    |    |    |-- element: array (containsNull = true)\n",
            " |    |    |    |    |    |-- element: double (containsNull = true)\n",
            " |    |    |-- type: string (nullable = true)\n",
            " |    |-- country_code: string (nullable = true)\n",
            " |    |-- id: string (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- place_type: string (nullable = true)\n",
            " |-- retweeted_status: struct (nullable = true)\n",
            " |    |-- _id: string (nullable = true)\n",
            " |    |-- user: struct (nullable = true)\n",
            " |    |    |-- followers_count: long (nullable = true)\n",
            " |    |    |-- friends_count: long (nullable = true)\n",
            " |    |    |-- id_str: string (nullable = true)\n",
            " |    |    |-- lang: string (nullable = true)\n",
            " |    |    |-- screen_name: string (nullable = true)\n",
            " |    |    |-- statuses_count: long (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- user: struct (nullable = true)\n",
            " |    |-- followers_count: long (nullable = true)\n",
            " |    |-- friends_count: long (nullable = true)\n",
            " |    |-- id_str: string (nullable = true)\n",
            " |    |-- lang: string (nullable = true)\n",
            " |    |-- screen_name: string (nullable = true)\n",
            " |    |-- statuses_count: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tweets.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "_2aAnfH5pNTB"
      },
      "source": [
        "### **Parte 2.2:** Más allá de las transformaciones SQL\n",
        "\n",
        "Algunas veces vamos a necesitar obtener resultados que precisan operaciones que van más allá de lo que podemos conseguir utilizando el lenguaje SQL. En esta parte de la práctica vamos practicar cómo pasar de una tabla a un RDD, para hacer operaciones complejas, y luego volver a pasar a una tabla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "Sbb-BsLApNTB"
      },
      "source": [
        "#### **Parte 2.2.1:** Tweets por población\n",
        "##### **Parte 2.2.1.1:** Utilizando SQL\n",
        "Un pequeño porcentaje, alrededor del 1%, de los tweets realizados está geolocalizado. Eso quiere decir que para estos tweets tenemos información acerca del lugar donde han sido realizados guardado en el campo ```place```. En este ejercicio se pide que utilizando una sentencia SQL mostréis en orden descendente cuántos tweets se han realizado en cada lugar. La tabla resultante ```tweets_place``` debe tener las siguientes columnas:\n",
        "- ***name:*** nombre del lugar\n",
        "- ***tweets:*** número de tweets\n",
        "\n",
        "Recordad que no todos los tweets en la base de datos tienen que tener información geolocalizada, tenéis que filtrarlos teniendo en cuenta todos los que tienen un valor no nulo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xBWxErVpNTB",
        "outputId": "51a211ea-9b9a-42ec-ab8a-fc629e1cf43e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------+\n",
            "|       name|tweets|\n",
            "+-----------+------+\n",
            "|     Madrid| 19655|\n",
            "|  Barcelona| 13987|\n",
            "|    Sevilla|  3820|\n",
            "|   Valencia|  2833|\n",
            "|   Zaragoza|  2449|\n",
            "|Villamartín|  2364|\n",
            "|     Málaga|  2184|\n",
            "|     Murcia|  1800|\n",
            "|    Granada|  1637|\n",
            "|   Alicante|  1628|\n",
            "+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hiveContext.sql(\"DROP TABLE IF EXISTS tweets\")\n",
        "hiveContext.registerDataFrameAsTable(tweets,\"tweets\")\n",
        "tweets_place = hiveContext.sql(\"SELECT place.name, COUNT(*) AS tweets FROM tweets WHERE place.name is not NULL GROUP BY place.name ORDER BY COUNT(*) DESC\")\n",
        "tweets_place.limit(10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g6T1VxdpNTB"
      },
      "outputs": [],
      "source": [
        "output = tweets_place.first()\n",
        "assert output.name == \"Madrid\" and output.tweets == 19655, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S8mrP2ZpNTC"
      },
      "source": [
        "##### **Parte 2.2.1.2:** Utilizando RDD\n",
        "\n",
        "Ahora se os pide que hagáis lo mismo pero esta vez utilizando RDD para realizar la agregación (recordad los ejercicios de contar palabras que hicisteis en la PEC 1).\n",
        "\n",
        "El primer paso consiste en generar un tabla ```tweets_geo``` que solo contenga información de tweets geolocalizados con una sola columna:\n",
        "- ***name:*** nombre del lugar desde donde se ha generado el tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJjJvyPlpNTC"
      },
      "outputs": [],
      "source": [
        "tweets_geo = hiveContext.sql(\"SELECT place.name FROM tweets WHERE place.name is not NULL \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU1bFjE3pNTC"
      },
      "source": [
        "Ahora viene la parte interesante. Una tabla puede convertirse en un RDD a través del atributo ```.rdd```. Este atributo guarda la información de la tabla en una lista donde cada elemento es un [objeto del tipo ```Row```](https://spark.apache.org/docs/1.1.1/api/python/pyspark.sql.Row-class.html). Los objetos pertenecientes a esta clase pueden verse como diccionarios donde la información de las diferentes columnas queda reflejada en forma de atributo. Por ejemplo, imaginad que tenemos una tabla con dos columnas, nombre y apellido, si utilizamos el atributo ```.rdd``` de dicha tabla obtendremos una lista con objetos del tipo row donde cada objeto tiene dos atributos: nombre y apellido. Para acceder a los atributos solo tenéis que utilizar la sintaxis *punto* de Python, e.g., ```row.nombre``` o ```row.apellido```.\n",
        "\n",
        "En esta parte del ejercicio se os pide que creeis un objeto ```tweets_lang_rdd``` que contenga una lista de tuplas con la información ```(name, tweets)``` sobre el nombre del lugar y el número de tweets generados desde allí. Recordad el ejercicio de contar palabras de la PEC 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVptUc3qpNTC",
        "outputId": "37963812-82a0-44c0-ec0e-9d0c149ee071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Altafulla', 49),\n",
              " ('Maryville', 1),\n",
              " ('Zamudio', 3),\n",
              " ('Aschaffenburg', 1),\n",
              " ('Summerlin South', 1),\n",
              " ('Sanxenxo', 23),\n",
              " ('San José Obrero', 1),\n",
              " ('Wiltz', 2),\n",
              " ('Santomera', 4),\n",
              " ('宇部市', 1),\n",
              " ('Alcalá de la Selva', 4),\n",
              " ('Greenville', 40),\n",
              " ('Erandio', 25),\n",
              " ('Platja de Gavà', 1),\n",
              " ('Boxmeer', 1),\n",
              " ('Valencina de la Concepción', 14),\n",
              " ('Vilallonga del Camp', 7),\n",
              " ('Escola Pública Ildefons Cerdà, Escola de Baix', 4),\n",
              " ('Panticosa', 2),\n",
              " ('Talca', 15)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_place_rdd = tweets_geo.rdd.map(list).map(lambda x: x[0]).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
        "tweets_place_rdd.take(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4poCtdtLpNTC"
      },
      "source": [
        "Una vez generado este RDD vamos a crear un tabla. El primer paso es generar por cada tupla un objeto Row que contenga un atributo ```name``` y un atributo ```tweets```. Ahora solo tenéis que aplicar el método ```toDF()``` para generar una tabla. Ordenad las filas de esta tabla por el número de tweets en orden descendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbedx61VpNTC",
        "outputId": "887707e7-7efc-494b-9399-47b362bdba34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------+\n",
            "|       name|tweets|\n",
            "+-----------+------+\n",
            "|     Madrid| 19655|\n",
            "|  Barcelona| 13987|\n",
            "|    Sevilla|  3820|\n",
            "|   Valencia|  2833|\n",
            "|   Zaragoza|  2449|\n",
            "|Villamartín|  2364|\n",
            "|     Málaga|  2184|\n",
            "|     Murcia|  1800|\n",
            "|    Granada|  1637|\n",
            "|   Alicante|  1628|\n",
            "+-----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tweets_place = tweets_place_rdd.map(lambda t: Row(name=t[0], tweets=t[1])).toDF().orderBy('tweets', ascending=False)\n",
        "\n",
        "tweets_place.limit(10).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzVTEsz7pNTC"
      },
      "outputs": [],
      "source": [
        "output = tweets_place.first()\n",
        "assert output.name == \"Madrid\" and output.tweets == 19655, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvJIZLCApNTC"
      },
      "source": [
        "#### **Parte 2.2.2:** Contar hashtags\n",
        "\n",
        "En el ejercicio anterior hemos visto cómo podemos generar la misma información haciendo una agregación mediante SQL o utilizando RDDs. Como seguro que habéis observado la semántica de la sentencia SQL es mucho más limpia para realizar esta tarea. Pero no todas las tareas que os vais a encontrar se pueden hacer mediante sentencias SQL. En este ejercicio vamos a ver un ejemplo.\n",
        "\n",
        "El objetivo de este ejercicio es contar el número de veces que cada hashtag (palabras precedidas por un #) ha aparecido en el dataset. Para evitar la sobrerrepresentación debida a los retweets vamos a concentrarnos en solo aquellos tweets que no son retweets de ningún otro, o dicho de otra manera, en aquellos en los que el campo ```retweeted_status``` es nulo. Cread una variable ```non_retweets``` que contenga todos estos tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwocC5WKpNTD",
        "outputId": "4e84f1b7-9c42-4f4d-e73a-452f66808b59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['@Pablo_Iglesias_',\n",
              " '@MiguelTheFaker',\n",
              " 'Pablo',\n",
              " 'ya',\n",
              " 'verás',\n",
              " 'como',\n",
              " 'se',\n",
              " 'entere',\n",
              " 'más',\n",
              " 'de']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "#.map(lambda s: s.replace(\"…\",\"\")).map(deEmojify)\\\n",
        "#.map(lambda s: re.sub('#',' #', s))\n",
        "non_retweets = hiveContext.sql(\"SELECT text FROM tweets WHERE retweeted_status.user is null\").rdd.map(list)\\\n",
        "                          .map(lambda x: x[0]).map(lambda s: re.sub('#',' #', s))\\\n",
        "                          .map(lambda x: x.split()).flatMap(lambda xs: [x for x in xs])\n",
        "#non_retweets = tweets.where(\"retweeted_status.user.statuses_count == '0'\").select(\"text\")\n",
        "non_retweets.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R3b3INLpNTD"
      },
      "source": [
        "Seguidamente vamos ha crear una variable ```hashtags``` que contenga una lista de tuplas con la información ```(hashtag, count)```. Para ello, cread un RDD que contenga una lista con el texto de todos los tweets. Una vez hecho este paso tenéis que extraer los hashtags (palabras precedidas por un #) y contarlos.\n",
        "\n",
        "Recordad los conocimientos adquiridos en la PEC 1 y el anterior ejercicio, os serán de gran ayuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6icExlopNTD",
        "outputId": "723d4a07-5581-492f-8351-187340de96a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('#039', 139),\n",
              " ('#laEspañaquequiero', 2),\n",
              " ('#PucherazoNo', 1),\n",
              " ('#checknews', 1),\n",
              " ('#elizabethwarren', 6),\n",
              " ('#amrkplaying', 1),\n",
              " ('#BADAJOZ', 1),\n",
              " ('#ThisistheRealSpain', 1),\n",
              " ('#VotadInsensatos', 2),\n",
              " ('#ingresos', 4)]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hashtags = non_retweets.filter(lambda x: x.startswith('#')).map(lambda x: x.strip('.,:?|!;\"-)\"')).map(lambda x: re.sub(r'\\W+', ' ',x)).map(lambda s: re.sub(' ','', s)).map(lambda s: s.strip()).map(lambda s: '#'+s).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
        "hashtags.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6BqajXGpNTD"
      },
      "source": [
        "Finalmente, se os pide que con el RDD obtenido generéis una tabla ```hashtagsTable``` compuesta de dos columnas:\n",
        "- ***hashtag***\n",
        "- ***num:*** número de veces que aparece cada hashtag.\n",
        "\n",
        "Ordenadla en orden descendente por número de tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZQt9_f8pNTD",
        "outputId": "ebfd114f-7404-48e7-dac4-142c19540035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|             hashtag|   num|\n",
            "+--------------------+------+\n",
            "|                #28A|158018|\n",
            "|   #ElDebateDecisivo|108514|\n",
            "|     #ELDEBATEenRTVE| 94192|\n",
            "|#EleccionesGenera...| 33809|\n",
            "|     #EquiparacionYa| 30563|\n",
            "|       #EleccionesL6| 30058|\n",
            "|         #HazQuePase| 26512|\n",
            "|   #DebateAtresmedia| 21849|\n",
            "|         #DebateRTVE| 17704|\n",
            "|#LaHistoriaLaEscr...| 16958|\n",
            "|          #PorEspaña| 16166|\n",
            "|           #DebatTV3| 14693|\n",
            "|#EleccionesGenerales| 14439|\n",
            "|                #28a| 14320|\n",
            "|         #ILPJusapol| 13487|\n",
            "|            #28Abril| 12839|\n",
            "|         #EspañaViva| 12619|\n",
            "|           #VotaPSOE| 12279|\n",
            "|        #ValorSeguro| 11133|\n",
            "|                #VOX| 10695|\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hashtagsTable = hashtags.map(lambda t: Row(hashtag=t[0], num=t[1])).toDF().orderBy('num', ascending=False)\n",
        "\n",
        "\n",
        "hashtagsTable.limit(20).show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf18JiR4pNTD"
      },
      "outputs": [],
      "source": [
        "output = hashtagsTable.first()\n",
        "assert output.hashtag == \"#28A\" and output.num >= 158000, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "8Pr5uTOVpNTE"
      },
      "source": [
        "## **Part 3:** Sampling\n",
        "\n",
        "In many occasions, before launching expensive processes, it is a common practice to deal with a small set of data to investigate some properties or simply to debug our algorithms, this task is called sampling. In this part of the practice we are going to look at the two main sampling methods and how to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYAIdRompNTE"
      },
      "source": [
        "### **Part 3.1:** Homogeneous\n",
        "\n",
        "The first sampling that we are going to see is [the homogeneous] (https://en.wikipedia.org/wiki/Simple_random_sample). This sampling is enough to simply choose a fraction of the population by randomly selecting elements from it.\n",
        "\n",
        "First of all, we are going to carry out a homogeneous sampling of 1% of the tweets generated in the electoral period without replacement. Save in a variable ```tweets_sample``` this sample using the method ``` sample``` described in the [pyspark SQL API] (https://spark.apache.org/docs/latest/api/python /pyspark.sql.html). The seed that you are going to use to initialize the random generator is 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gALWA3bdpNTE",
        "outputId": "3f231f8d-c868-49f9-b4ce-948030a6a1d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tweets sampled: 254185\n"
          ]
        }
      ],
      "source": [
        "seed = 42\n",
        "fraction = 0.01\n",
        "\n",
        "tweets_sample = tweets.sample(fraction, seed)\n",
        "\n",
        "print(\"Number of tweets sampled: {0}\".format(tweets_sample.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRv-44JrpNTE"
      },
      "outputs": [],
      "source": [
        "assert tweets_sample.count() == 254185, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymvg2df_pNTE"
      },
      "source": [
        "Una de las cosas que resulta interesante comprobar acerca de los patrones de uso de las redes sociales es el patrón de uso diario. En este caso nos interesa el número promedio de tweets que se genera cada hora del día. Para extraer esta información lo que haremos primero, será generar una tabla ```tweets_timestamp``` con la información:\n",
        "- ***created_at***: timestamp de cuando se publicó el tweet.\n",
        "- ***hour***: a que hora del dia corresponde.\n",
        "- ***day***: Fecha en formato MM-dd-YY\n",
        "\n",
        "La fecha que figura en la base de datos esta en la franja horaria GMT. El primer paso es pasar esta información al horario peninsular de España, podéis utilizar la función ```from_utc_timestamp``` para este fin. Así mismo, la función ```hour``` os servirá para extraer la hora del timestamp y la función ```date_format``` os permitirá generar la fecha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nvl7W25pNTE",
        "outputId": "ab62ad30-3afe-4a02-8bb7-379ec23f959e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+--------+\n",
            "|         created_at|hour|     day|\n",
            "+-------------------+----+--------+\n",
            "|2019-04-21 04:24:26|   2|04-21-19|\n",
            "|2019-04-21 04:24:44|   2|04-21-19|\n",
            "|2019-04-21 04:24:46|   2|04-21-19|\n",
            "|2019-04-21 04:25:50|   2|04-21-19|\n",
            "|2019-04-21 04:25:53|   2|04-21-19|\n",
            "|2019-04-21 04:25:59|   2|04-21-19|\n",
            "|2019-04-21 04:26:21|   2|04-21-19|\n",
            "|2019-04-21 04:27:31|   2|04-21-19|\n",
            "|2019-04-21 04:28:01|   2|04-21-19|\n",
            "|2019-04-21 04:28:09|   2|04-21-19|\n",
            "|2019-04-21 04:28:14|   2|04-21-19|\n",
            "|2019-04-21 04:28:21|   2|04-21-19|\n",
            "|2019-04-21 04:28:35|   2|04-21-19|\n",
            "|2019-04-21 04:28:53|   2|04-21-19|\n",
            "|2019-04-21 04:29:10|   2|04-21-19|\n",
            "|2019-04-21 04:29:34|   2|04-21-19|\n",
            "|2019-04-21 04:29:39|   2|04-21-19|\n",
            "|2019-04-21 04:29:56|   2|04-21-19|\n",
            "|2019-04-21 04:30:06|   2|04-21-19|\n",
            "|2019-04-21 04:30:25|   2|04-21-19|\n",
            "+-------------------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import date_format, hour, from_utc_timestamp\n",
        "\n",
        "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_sample\")\n",
        "sqlContext.registerDataFrameAsTable(tweets_sample,\"tweets_sample\")\n",
        "\n",
        "tweets_timestamp = sqlContext.sql(\"\"\" SELECT from_utc_timestamp(created_at, 'Europe/Madrid') as created_at\n",
        "                         ,hour(created_at) as hour\n",
        "                         ,date_format(created_at, 'MM-dd-YY') as day\n",
        "                               FROM tweets_sample\n",
        "                               \"\"\")\n",
        "\n",
        "\n",
        "tweets_timestamp.limit(20).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma2UZOmYpNTE"
      },
      "source": [
        "El paso siguiente es agregar estos datos por hora y día en una tabla ```tweets_hour_day```. Tenéis que crear una tabla ```tweets_hour``` con la información:\n",
        "- ***hour:*** hora del dia\n",
        "- ***day:*** fecha\n",
        "- ***count:*** número de tweets generados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90GJgPNIpNTE",
        "outputId": "b3379b28-b589-4e38-c174-ffbf7a8b1a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------+-----+\n",
            "|hour|     day|count|\n",
            "+----+--------+-----+\n",
            "|  18|04-13-19|  536|\n",
            "|  11|04-17-19|  529|\n",
            "|  10|04-28-19|  770|\n",
            "|  23|04-23-19| 4893|\n",
            "|  11|04-28-19|  954|\n",
            "|   7|04-22-19|  184|\n",
            "|  16|04-24-19| 1000|\n",
            "|   5|04-28-19|   76|\n",
            "|   6|04-29-19|  289|\n",
            "|   4|04-17-19|   69|\n",
            "|   5|04-27-19|   97|\n",
            "|  10|04-14-19|  489|\n",
            "|  23|04-26-19| 1118|\n",
            "|   7|04-17-19|  275|\n",
            "|  13|04-25-19|  697|\n",
            "|  20|04-28-19| 1644|\n",
            "|   0|04-23-19| 2789|\n",
            "|   6|04-23-19|  226|\n",
            "|   7|04-26-19|  355|\n",
            "|   8|04-21-19|  261|\n",
            "+----+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_timestamp\")\n",
        "sqlContext.registerDataFrameAsTable(tweets_timestamp,\"tweets_timestamp\")\n",
        "tweets_hour_day = sqlContext.sql(\"\"\" SELECT hour,day, COUNT(*) AS count\n",
        "                               FROM tweets_timestamp\n",
        "                               GROUP BY hour,day\n",
        "                               \"\"\")\n",
        "\n",
        "tweets_hour_day.limit(20).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMw2ZpupNTF"
      },
      "source": [
        "Por último solo nos queda hacer una agregación por hora para conseguir el promedio de tweets por hora. Tenéis que generar una tabla ```tweets_hour``` con la información:\n",
        "- ***hour:*** Hora\n",
        "- ***tweets:*** Promedio de tweets realizados\n",
        "\n",
        "Recordad que estamos trabajando con un sample del 1% por tanto tenéis que corregir la columna ```tweets``` para que refleje el promedio que deberíamos esperar en el conjunto completo de tweets. La tabla tiene que estar ordenada en orden ascendente de hora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-syugP-_pNTF",
        "outputId": "5e84a1f4-3b7f-4abb-edcb-8232bd26a799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------------------+\n",
            "|hour|            tweets|\n",
            "+----+------------------+\n",
            "|   0|105005.55555555556|\n",
            "|   1|51766.666666666664|\n",
            "|   2| 23115.78947368421|\n",
            "|   3|13094.117647058823|\n",
            "|   4| 9370.588235294117|\n",
            "|   5|  8347.05882352941|\n",
            "|   6|11666.666666666668|\n",
            "|   7|25627.777777777777|\n",
            "|   8|42523.529411764706|\n",
            "|   9| 52111.11111111111|\n",
            "|  10|58344.444444444445|\n",
            "|  11|           61350.0|\n",
            "|  12| 62188.88888888889|\n",
            "|  13| 66077.77777777778|\n",
            "|  14| 67855.55555555555|\n",
            "|  15| 71494.44444444445|\n",
            "|  16| 67794.44444444445|\n",
            "|  17| 63538.88888888889|\n",
            "|  18| 67555.55555555555|\n",
            "|  19| 68211.11111111111|\n",
            "+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_hour_day\")\n",
        "sqlContext.registerDataFrameAsTable(tweets_hour_day,\"tweets_hour_day\")\n",
        "\n",
        "tweets_hour = sqlContext.sql(\"\"\" SELECT hour, avg(count)*100 as tweets\n",
        "                               FROM tweets_hour_day\n",
        "                               GROUP BY hour\n",
        "                               ORDER BY hour\n",
        "                               \"\"\")\n",
        "\n",
        "tweets_hour.limit(24).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUL8BParpNTF"
      },
      "source": [
        "Por último, tenéis que producir un gráfico de barras utilizando Pandas donde se muestre la información que acabáis de generar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIJwY1zupNTF",
        "outputId": "3f1a396c-a1cb-4596-8474-d1e336e13f4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f83e59f3080>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "tweets_hour_pd = tweets_hour.toPandas()\n",
        "tweets_hour_pd.plot.bar(x='hour', y='tweets')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToVT_DCfpNTF"
      },
      "source": [
        "### **Parte 3.2:** Estratificado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChaUxbRepNTF"
      },
      "source": [
        "In many cases, homogeneous sampling is not adequate since, due to the structure of the data, certain segments may be over-represented. This is the case that we observe in the tweets where large urban areas are overrepresented if we compare it with the volume of the population. In this activity we will see how to apply this technique to the tweet dataset, to obtain a sample that respects the proportion of deputies per province.\n",
        "\n",
        "In Spain, the electoral process assigns a volume of deputies to each province that depends on the population and a minimum percentage assigned by law. In the Hive context that we have previously created (```hiveContext```) we can find a table (``` province_28a```) containing information about electoral districts. Load this table into a variable named ```province```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpc5CO3SpNTG",
        "outputId": "251a2cdc-6451-4fdc-e8b9-94d84f09306b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+------------------+----------+---------+\n",
            "|    capital|   province|              ccaa|population|diputados|\n",
            "+-----------+-----------+------------------+----------+---------+\n",
            "|     Teruel|     Teruel|            Aragón|     35691|        3|\n",
            "|      Soria|      Soria|   Castilla y León|     39112|        2|\n",
            "|    Segovia|    Segovia|   Castilla y León|     51683|        3|\n",
            "|     Huesca|     Huesca|            Aragón|     52463|        3|\n",
            "|     Cuenca|     Cuenca|Castilla-La Mancha|     54898|        3|\n",
            "|      Ávila|      Ávila|   Castilla y León|     57697|        3|\n",
            "|     Zamora|     Zamora|   Castilla y León|     61827|        3|\n",
            "|Ciudad Real|Ciudad Real|Castilla-La Mancha|     74743|        5|\n",
            "|   Palencia|   Palencia|   Castilla y León|     78629|        3|\n",
            "| Pontevedra| Pontevedra|           Galicia|     82802|        7|\n",
            "|     Toledo|     Toledo|Castilla-La Mancha|     84282|        6|\n",
            "|Guadalajara|Guadalajara|Castilla-La Mancha|     84910|        3|\n",
            "|      Ceuta|      Ceuta|             Ceuta|     85144|        1|\n",
            "|    Melilla|    Melilla|           Melilla|     86384|        1|\n",
            "|    Cáceres|    Cáceres|       Extremadura|     96098|        4|\n",
            "|       Lugo|       Lugo|           Galicia|     98025|        4|\n",
            "|     Girona|     Girona|          Cataluña|    100266|        6|\n",
            "|     Orense|     Orense|           Galicia|    105505|        4|\n",
            "|       Jaén|       Jaén|         Andalucía|    113457|        5|\n",
            "|      Cádiz|      Cádiz|         Andalucía|    116979|        9|\n",
            "+-----------+-----------+------------------+----------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "province = hiveContext.table('province_28a')\n",
        "province.limit(20).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgeblVyQpNTG"
      },
      "source": [
        "Para hacer un sampling estratificado lo primero que tenemos que hacer es determinar la fracción que queremos asignar a cada categoría. En este caso queremos una fracción que haga que el ratio tweets diputado sea igual para todas las capitales de provincia. Tenemos que tener en cuenta que la precisión de la geolocalización en Twitter és normalmente a nivel de ciudad. Por eso, para evitar incrementar la complejidad del ejercicio, vamos a utilizar los tweets en capitales de provincia como proxy de los tweets en toda la provincia.\n",
        "\n",
        "Lo primero que tenéis que hacer es crear un tabla ```info_tweets_province``` que debe contener:\n",
        "- ***capital:*** nombre de la capital de provincia.\n",
        "- ***tweets:*** número de tweets geolocalizados en cada capital\n",
        "- ***diputados:*** diputados que asignados a la provincia.\n",
        "- ***ratio_tweets_diputado:*** número de tweets por diputado.\n",
        "\n",
        "Debéis ordenar la lista por ```ratio_tweets_diputado``` en orden ascendente.\n",
        "\n",
        "***Nota:*** Podéis realizar este ejercicio de muchas maneras, probablemente la más fácil es utilizar la tabla ```tweets_place``` que habéis generado en el apartado 2.2.1. Recordad cómo utilizar el ```join()```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e-HnUaBpNTG",
        "outputId": "ca622968-546e-4792-f8ea-8de9c5fe0f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+---------+---------------------+\n",
            "|             capital|tweets|diputados|ratio_tweets_diputado|\n",
            "+--------------------+------+---------+---------------------+\n",
            "|              Teruel|    35|        3|   11.666666666666666|\n",
            "|          Pontevedra|   154|        7|                 22.0|\n",
            "|              Huesca|    85|        3|   28.333333333333332|\n",
            "|              Zamora|    94|        3|   31.333333333333332|\n",
            "|               Soria|    79|        2|                 39.5|\n",
            "|             Segovia|   119|        3|   39.666666666666664|\n",
            "|              Cuenca|   146|        3|   48.666666666666664|\n",
            "|               Cádiz|   453|        9|   50.333333333333336|\n",
            "|         Ciudad Real|   276|        5|                 55.2|\n",
            "|            Pamplona|   281|        5|                 56.2|\n",
            "|                Lugo|   229|        4|                57.25|\n",
            "|Santa Cruz de Ten...|   471|        7|    67.28571428571429|\n",
            "|                Jaén|   356|        5|                 71.2|\n",
            "|             Cáceres|   288|        4|                 72.0|\n",
            "|       San Sebastián|   465|        6|                 77.5|\n",
            "|              Toledo|   494|        6|    82.33333333333333|\n",
            "|             Badajoz|   510|        6|                 85.0|\n",
            "|         Guadalajara|   264|        3|                 88.0|\n",
            "|             Almería|   529|        6|    88.16666666666667|\n",
            "|            Albacete|   371|        4|                92.75|\n",
            "+--------------------+------+---------+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#<FILL IN>\n",
        "sqlContext.sql(\"DROP TABLE IF EXISTS province\")\n",
        "sqlContext.registerDataFrameAsTable(province,\"province\")\n",
        "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_place\")\n",
        "sqlContext.registerDataFrameAsTable(tweets_place,\"tweets_place\") \n",
        "info_tweets_province = hiveContext.sql(\"\"\"SELECT b.capital, a.tweets, b.diputados, a.tweets/b.diputados as ratio_tweets_diputado\n",
        "                                    FROM tweets_place a\n",
        "                                    INNER JOIN province b on a.name = b.capital\n",
        "                                    ORDER BY  a.tweets/b.diputados\n",
        "                                    \"\"\")\n",
        "\n",
        "\n",
        "info_tweets_province.limit(20).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY0Nu15kpNTG"
      },
      "outputs": [],
      "source": [
        "output = info_tweets_province.first()\n",
        "maximum_ratio = floor(output.ratio_tweets_diputado * 100) / 100\n",
        "\n",
        "assert output.capital == \"Teruel\" and output.tweets == 35 and output.diputados == 3 and maximum_ratio == 11.66, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGDPV2bXpNTG"
      },
      "source": [
        "Lo primero que vamos a necesitar es un diccionario con nombre ```ratios``` donde cada capital de provincia es una llave y su valor asociado es la fracción de tweets que vamos a samplear. En este caso lo que queremos es que el ratio de tweets por cada diputado sea similar para cada capital de provincia.\n",
        "\n",
        "Como queremos que el sampling sea lo más grande posible y no queremos que ninguna capital este infrarepresentada el ratio de tweets por diputado será el valor más pequeño podéis observar en la tabla ```info_tweets_province```, que corresponde a 11.66 tweets por diputado en Teruel. Tenéis este valor guardado en la variable ```maximum_ratio```.\n",
        "\n",
        "*Nota:* El método ```collectAsMap()``` transforma un PairRDD en un diccionario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYsg9-17pNTG",
        "outputId": "19811cd9-fa17-44ff-c219-6c891d398fa7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Albacete': 0.12578616280323449,\n",
              " 'Alicante': 0.08599508550368551,\n",
              " 'Almería': 0.13232514102079393,\n",
              " 'Badajoz': 0.13725490117647057,\n",
              " 'Barcelona': 0.02669145143347394,\n",
              " 'Bilbao': 0.1078998067052023,\n",
              " 'Burgos': 0.0895713366602687,\n",
              " 'Castellón de la Plana': 0.11437908431372548,\n",
              " 'Ceuta': 0.0804597696551724,\n",
              " 'Ciudad Real': 0.2113526557971014,\n",
              " 'Cuenca': 0.23972602602739726,\n",
              " 'Cáceres': 0.1620370361111111,\n",
              " 'Cádiz': 0.23178807814569533,\n",
              " 'Córdoba': 0.0667938927480916,\n",
              " 'Girona': 0.07314524514106582,\n",
              " 'Granada': 0.0498880062309102,\n",
              " 'Guadalajara': 0.13257575681818182,\n",
              " 'Huelva': 0.09244585261489698,\n",
              " 'Huesca': 0.41176470352941175,\n",
              " 'Jaén': 0.1638576769662921,\n",
              " 'Las Palmas de Gran Canaria': 0.0723514207751938,\n",
              " 'León': 0.0846944943738657,\n",
              " 'Lleida': 0.09079117976653696,\n",
              " 'Logroño': 0.10727969287356322,\n",
              " 'Lugo': 0.2037845694323144,\n",
              " 'Madrid': 0.021962180829305518,\n",
              " 'Melilla': 0.02615844529147982,\n",
              " 'Murcia': 0.06481481444444444,\n",
              " 'Málaga': 0.05876068342490842,\n",
              " 'Oviedo': 0.06196256919575114,\n",
              " 'Palencia': 0.11075949303797468,\n",
              " 'Palma': 0.05844291346274264,\n",
              " 'Pamplona': 0.20759193238434162,\n",
              " 'Pontevedra': 0.5303030272727273,\n",
              " 'Salamanca': 0.058187863341645885,\n",
              " 'San Sebastián': 0.1505376335483871,\n",
              " 'Santa Cruz de Tenerife': 0.1733899494692144,\n",
              " 'Santander': 0.07192766091245376,\n",
              " 'Segovia': 0.29411764537815127,\n",
              " 'Sevilla': 0.03664921445026178,\n",
              " 'Soria': 0.29535864810126583,\n",
              " 'Tarragona': 0.11290322516129032,\n",
              " 'Teruel': 0.9999999942857143,\n",
              " 'Toledo': 0.141700404048583,\n",
              " 'Valencia': 0.0617719728203318,\n",
              " 'Valladolid': 0.04585953852201258,\n",
              " 'Vitoria-Gasteiz': 0.12511170616621983,\n",
              " 'Zamora': 0.3723404234042553,\n",
              " 'Zaragoza': 0.0333469441404655,\n",
              " 'Ávila': 0.09043927596899225}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#province > capital, maximum_ratio/diputados\n",
        "\n",
        "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_timestamp\") #freeing memory\n",
        "sqlContext.sql(\"DROP TABLE IF EXISTS info_tweets_province\")\n",
        "sqlContext.registerDataFrameAsTable(info_tweets_province,\"info_tweets_province\")\n",
        "ratios = hiveContext.sql(\"SELECT DISTINCT capital, 11.6666666/ratio_tweets_diputado AS ratio_tweets_diputado FROM info_tweets_province\").rdd.collectAsMap()\n",
        "ratios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkrmscappNTG"
      },
      "source": [
        "Generad una tabla ```geo_tweets``` con los tweets geolocalizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU4PoSGfpNTG"
      },
      "outputs": [],
      "source": [
        "geo_tweets = hiveContext.sql(\"SELECT place.name, text FROM tweets WHERE place.name is not NULL\")\n",
        "#print((geo_tweets.count(), len(geo_tweets.columns)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TKV6QSppNTH"
      },
      "source": [
        "Ahora ya estamos en disposición de hacer el sampling estratificado por población. Para ello podéis utilizar el método ```sampleBy()```. Utilizad 42 como seed del generador pseudoaleatorio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K8c2_ZTpNTH"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "sample = geo_tweets.sampleBy(\"name\", fractions=ratios, seed=seed)\n",
        "#print((sample.count(), len(sample.columns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmhcj00SpNTH"
      },
      "source": [
        "Para visualizar el resultado del sampling vais a crear una tabla ```info_sample``` que contenga la siguiente información:\n",
        "- ***capital:*** nombre de la capital de provincia.\n",
        "- ***tweets:*** número de tweets sampleados en cada capital\n",
        "- ***diputados:*** diputados que asignados a la provincia.\n",
        "- ***ratio_tweets_diputado:*** número de tweets por diputado.\n",
        "\n",
        "Ordenad la tabla resultante por orden de ```ratio_tweets_diputado```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9L68ZkbpNTH",
        "outputId": "e133ef74-50de-4695-d808-1a196438a148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+---------+------+---------------------+\n",
            "|        capital|diputados|tweets|ratio_tweets_diputado|\n",
            "+---------------+---------+------+---------------------+\n",
            "|        Melilla|        1|     6|                  6.0|\n",
            "|          Ceuta|        1|     8|                  8.0|\n",
            "|    Ciudad Real|        5|    44|                  8.8|\n",
            "|           León|        4|    38|                  9.5|\n",
            "|       Albacete|        4|    39|                 9.75|\n",
            "|         Bilbao|        8|    80|                 10.0|\n",
            "|         Zamora|        3|    30|                 10.0|\n",
            "|    Guadalajara|        3|    30|                 10.0|\n",
            "|       Palencia|        3|    30|                 10.0|\n",
            "|       Alicante|       12|   122|   10.166666666666666|\n",
            "|      Tarragona|        6|    62|   10.333333333333334|\n",
            "|           Lugo|        4|    42|                 10.5|\n",
            "|        Segovia|        3|    32|   10.666666666666666|\n",
            "|         Toledo|        6|    64|   10.666666666666666|\n",
            "|       Pamplona|        5|    54|                 10.8|\n",
            "|           Jaén|        5|    54|                 10.8|\n",
            "|         Girona|        6|    65|   10.833333333333334|\n",
            "|         Madrid|       37|   403|   10.891891891891891|\n",
            "|Vitoria-Gasteiz|        4|    44|                 11.0|\n",
            "|      Santander|        5|    55|                 11.0|\n",
            "+---------------+---------+------+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"DROP TABLE IF EXISTS sample\")\n",
        "sqlContext.registerDataFrameAsTable(sample,\"sample\")\n",
        "\n",
        "\n",
        "\n",
        "#info_sample =  hiveContext.sql(\"SELECT COUNT(*), COUNT(DISTINCT capital) FROM info_tweets_province \")\n",
        "\n",
        "info_sample =  hiveContext.sql(\"\"\"SELECT DISTINCT capital,a.diputados, COUNT(*) as tweets, COUNT(*)/a.diputados AS ratio_tweets_diputado\n",
        "                                  FROM info_tweets_province a\n",
        "                                  INNER JOIN sample b on a.capital = b.name\n",
        "                                  GROUP BY capital, diputados\n",
        "                                  \"\"\").orderBy('ratio_tweets_diputado', ascending=True) \n",
        "#info_sample.printSchema()\n",
        "info_sample.limit(20).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8NWW97DpNTH"
      },
      "outputs": [],
      "source": [
        "\n",
        "output = info_sample.first()\n",
        "assert output.capital == \"Melilla\" and output.tweets == 6 and output.diputados == 1 and output.ratio_tweets_diputado == 6.0, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT4LeKrlpNTH"
      },
      "source": [
        "Como veis el sampling no es exacto, es una aproximación. Pero como podéis imaginar acercar el sampling a la representatividad electoral de las regiones son necesarios en muchos análisis.\n",
        "\n",
        "Para comprobarlo contad primero todos los hashtags presentes en la tabla ```geo_tweets``` tal como hemos hecho en el apartado 2.2.2 y ordenad el resultado por número de tweets en orden descendente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0_Ua02PpNTH",
        "outputId": "4ab35d0a-a95d-4ac9-8d4b-f425e0625fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----+\n",
            "|             hashtag| num|\n",
            "+--------------------+----+\n",
            "|                #28A|7411|\n",
            "|   #ElDebateDecisivo|3938|\n",
            "|     #ELDEBATEenRTVE|3510|\n",
            "|     #EquiparacionYa|2309|\n",
            "|       #6AbrilMadrid|1839|\n",
            "|         #ILPJusapoL|1762|\n",
            "|         #HazQuePase|1618|\n",
            "|#EleccionesGenera...|1478|\n",
            "|       #EleccionesL6| 988|\n",
            "|        #ValorSeguro| 864|\n",
            "|   #DebateAtresmedia| 847|\n",
            "|#LaHistoriaLaEscr...| 752|\n",
            "|           #VotaPSOE| 700|\n",
            "|         #DebateRTVE| 699|\n",
            "| #LaEspañaQueQuieres| 658|\n",
            "|#EleccionesGenerales| 653|\n",
            "|    #VamosCiudadanos| 598|\n",
            "|           #DebatTV3| 597|\n",
            "|             #España| 580|\n",
            "|            #28Abril| 570|\n",
            "+--------------------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "geo_tweets_text = geo_tweets.select(\"text\").rdd.map(list)\\\n",
        "                          .map(lambda x: x[0]).map(lambda s: re.sub('#',' #', s))\\\n",
        "                          .map(lambda x: x.split()).flatMap(lambda xs: [x for x in xs])\n",
        "#non_retweets = tweets.where(\"retweeted_status.user.statuses_count == '0'\").select(\"text\")\n",
        "#non_retweets.take(10)\n",
        "\n",
        "hashtags_geo = geo_tweets_text.filter(lambda x: x.startswith('#')).map(lambda x: x.strip('.,:?|!;\"-)\"')).map(lambda x: re.sub(r'\\W+', ' ',x)).map(lambda s: re.sub(' ','', s)).map(lambda s: s.strip()).map(lambda s: '#'+s).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "hashtagsTable_geo = hashtags_geo.map(lambda t: Row(hashtag=t[0], num=t[1])).toDF().orderBy('num', ascending=False)\n",
        "\n",
        "\n",
        "hashtagsTable_geo.limit(20).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xBnCLVkpNTH"
      },
      "source": [
        "Comparad este resultado con el que obtenemos cuando creamos una tabla ```hashtagsTable_sample``` donde contamos los hashtags en el sample. Ordenad la tabla por número de tweets en orden descendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuM6uwQtpNTH",
        "outputId": "c084f43b-8290-4e8e-c3bf-ab3620144192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---+\n",
            "|             hashtag|num|\n",
            "+--------------------+---+\n",
            "|                #28A|157|\n",
            "|   #ElDebateDecisivo| 95|\n",
            "|     #ELDEBATEenRTVE| 89|\n",
            "|#EleccionesGenera...| 32|\n",
            "|         #HazQuePase| 30|\n",
            "|       #EleccionesL6| 25|\n",
            "|         #DebateRTVE| 23|\n",
            "|        #ValorSeguro| 21|\n",
            "|#LaHistoriaLaEscr...| 20|\n",
            "|   #DebateAtresmedia| 19|\n",
            "|            #28Abril| 16|\n",
            "|    #VamosCiudadanos| 15|\n",
            "|      #UnidasPodemos| 14|\n",
            "|#EleccionesGenerales| 14|\n",
            "| #LaEspañaQueQuieres| 13|\n",
            "|           #VotaPSOE| 12|\n",
            "|#YoVotoUnidasPodemos| 12|\n",
            "|          #PorEspaña| 10|\n",
            "|           #DebatTV3| 10|\n",
            "|     #EquiparacionYa|  9|\n",
            "+--------------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_tweets_text = sample.select(\"text\").rdd.map(list)\\\n",
        "                          .map(lambda x: x[0]).map(lambda s: re.sub('#',' #', s))\\\n",
        "                          .map(lambda x: x.split()).flatMap(lambda xs: [x for x in xs])\n",
        "#non_retweets = tweets.where(\"retweeted_status.user.statuses_count == '0'\").select(\"text\")\n",
        "#non_retweets.take(10)\n",
        "\n",
        "hashtags_sample = sample_tweets_text.filter(lambda x: x.startswith('#')).map(lambda x: x.strip('.,:?|!;\"-)\"')).map(lambda x: re.sub(r'\\W+', ' ',x)).map(lambda s: re.sub(' ','', s)).map(lambda s: s.strip()).map(lambda s: '#'+s).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "hashtagsTable_sample = hashtags_sample.map(lambda t: Row(hashtag=t[0], num=t[1])).toDF().orderBy('num', ascending=False)\n",
        "\n",
        "\n",
        "hashtagsTable_sample.limit(20).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "97ap3X9HpNTI"
      },
      "source": [
        "## **Parte 4:** Introducción a los datos relacionales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQm6venpNTI"
      },
      "source": [
        "The fact of working with a database that contains information generated in a social network allows us to introduce the concept of relational data. We can define relational data as those in which there are relationships between the entities that make up the database. If these relations are binary, relations 1 to 1, we can represent the relations as a graph composed of a set of vertices $\\mathcal{V}$ and a set of edges $\\mathcal{E}$ that relate them.\n",
        "\n",
        "In the case of graphs that emerge organically, this type of structure goes beyond the regular graphs that you surely know. This type of structure is known as [complex networks](https://es.wikipedia.org/wiki/Red_compleja). The study of the structure and dynamics of this type of network has contributed to important results in fields as diverse as physics, sociology, ecology and medicine.\n",
        "\n",
        "![complex_network](https://images.squarespace-cdn.com/content/5150aec6e4b0e340ec52710a/1364574727391-XVOFAB9P6GHKTDAH6QTA/lastfm_800_graph_white.png?content-type=image%2Fpng)\n",
        "\n",
        "In this last part of the practice we are going to work with this type of data. Specifically, we are going to model one of the possible relationships present in the dataset, the retweet network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUNtxmCjpNTI"
      },
      "source": [
        "### **Parte 4.1:** Generar la red de retweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXW7yIl-pNTI"
      },
      "source": [
        "#### **Parte 4.1.1**: Construcción de la edgelist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfgyWqiepNTI"
      },
      "source": [
        "Lo primero se os pide es que generéis la red. Hay diversas maneras de representar una red compleja, por ejemplo, si estuvierais interesados en trabajar en ellas desde el punto de vista teórico, la manera más habitual de representarlas es utilizando una [matriz de adyacencia](https://es.wikipedia.org/wiki/Matriz_de_adyacencia). En esta práctica vamos a centrarnos en el aspecto computacional, una de las maneras de mas eficientes (computacionalmente hablando) de representar una red es mediante su [*edge list*](https://en.wikipedia.org/wiki/Edge_list), una tabla que especifica la relación a parejas entre las entidades.\n",
        "\n",
        "Las relaciones pueden ser bidireccionales o direccionales y tener algún peso asignado o no (weighted or unweighted). En el caso que nos ocupa, estamos hablando de una red dirigida, un usuario retuitea a otro, y podemos pensarla teniendo en cuenta cuántas veces esto ha pasado.\n",
        "\n",
        "Lo primero que haréis para simplificar el cómputo,  es crear un sample homogéneo sin reemplazo del 1% de los tweets. Utilizando los conocimientos que habéis aprendido en el apartado 3.1. Utilizaremos 42 como valor para la seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JONnQ6H8pNTI"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "fraction = 0.01\n",
        "\n",
        "sample = tweets.sample(fraction = fraction, seed = seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgDkIg8apNTI"
      },
      "source": [
        "Ahora vais a crear una tabla ```edgelist``` con la siguiente información:\n",
        "- ***src:*** usuario que retuitea\n",
        "- ***dst:*** usuario que es retuiteado\n",
        "- ***weight:*** número de veces que un usuario retuitea a otro.\n",
        "\n",
        "Filtrar el resultado para que contenga sólo las relaciones con un weight igual o mayor a dos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpwbbhVcpNTI",
        "outputId": "4cafb050-a429-43ce-e82d-096339923564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 5247 edges on the network.\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"DROP TABLE IF EXISTS sample\")\n",
        "sqlContext.registerDataFrameAsTable(sample,\"sample\")\n",
        "\n",
        "edgelist = sqlContext.sql(\"\"\" SELECT user.screen_name as src\n",
        "                            , retweeted_status.user.screen_name as dst\n",
        "                            , count(*) as weight\n",
        "                               FROM sample\n",
        "                               WHERE retweeted_status.user.statuses_count > '0'\n",
        "                               group by user.screen_name\n",
        "                            , retweeted_status.user.screen_name \n",
        "                            having count(*) > 1\n",
        "                               \"\"\")\n",
        "L = edgelist.count()\n",
        "\n",
        "print(\"There are {0} edges on the network.\".format(L))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKrWvs16pNTJ"
      },
      "outputs": [],
      "source": [
        "assert L == 5247, \"Incorrect ouput\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxzR7if6pNTJ"
      },
      "source": [
        "#### **Parte 4.1.2:** Centralidad de grado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tcPqEDNpNTJ"
      },
      "source": [
        "Uno de los descriptores más comunes en el análisis de redes es el grado. El grado cuantifica cuántas aristas están conectadas a cada vértices. En el caso de redes dirigidas como la que acabamos de crear este descriptor está descompuesto en el:\n",
        "- **in degree**: cuantas aristas apuntan al nodo\n",
        "- **out degree**: cuantas aristas salen del nodo\n",
        "\n",
        "Si haces un ranquing de estos valores vais a obtener medida de centralidad, la [centralidad de grado](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), de cada uno de los nodos.\n",
        "\n",
        "Se os pide que generéis una tabla con la información:\n",
        "- ***screen_name:*** nombre del usuario.\n",
        "- ***outDegree:*** out degree del nodo.\n",
        "\n",
        "Ordenado la tabla por out degree en orden descendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzS4ElEPpNTJ",
        "outputId": "7e72b865-d528-4e90-d5f0-9d7a92dd7723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+---------+\n",
            "|    screen_name|outDegree|\n",
            "+---------------+---------+\n",
            "|   rosavergar23|       11|\n",
            "|JulioAl18175505|       10|\n",
            "|      el_partal|       10|\n",
            "|    SSarelvis67|        9|\n",
            "|Teresaperezcep1|        8|\n",
            "|miguelgutiperez|        8|\n",
            "|       anap1958|        8|\n",
            "|      MACUBERNA|        7|\n",
            "|  yomismaconcha|        7|\n",
            "|        Fermirv|        7|\n",
            "|    pacomarina6|        7|\n",
            "|   Socialista60|        7|\n",
            "|     astroman78|        7|\n",
            "|       jasalo54|        7|\n",
            "|  Rafa_eltorete|        7|\n",
            "|        lyuva26|        7|\n",
            "|    mercedescdz|        6|\n",
            "|        crg1212|        6|\n",
            "|  joanagabarrof|        6|\n",
            "|     carrasquem|        6|\n",
            "+---------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "outDegree = edgelist.groupBy(\"src\")\\\n",
        "                 .agg(f.count('*'))\\\n",
        "                 .orderBy(\"count(1)\", ascending=0)\n",
        "outDegree = outDegree.withColumnRenamed(\"count(1)\", \"outDegree\")\n",
        "outDegree = outDegree.withColumnRenamed(\"src\", \"screen_name\")\n",
        "\n",
        "outDegree.limit(20).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5vPCnIBpNTK"
      },
      "outputs": [],
      "source": [
        "output = outDegree.first()\n",
        "assert output.screen_name == \"rosavergar23\" and output.outDegree == 11, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zhomg18pNTK"
      },
      "source": [
        "Se os pide ahora que generéis una tabla con la información:\n",
        "- ***screen_name:*** nombre del usuario.\n",
        "- ***inDegree:*** in degree del nodo.\n",
        "\n",
        "Ordenad la tabla por in degree en orden descendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyHOImXWpNTK",
        "outputId": "548f3654-a177-432f-bf8d-28e49d71d6dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+--------+\n",
            "|    screen_name|inDegree|\n",
            "+---------------+--------+\n",
            "|         vox_es|     330|\n",
            "|   ahorapodemos|     279|\n",
            "|           PSOE|     242|\n",
            "|   CiudadanosCs|     218|\n",
            "|  Santi_ABASCAL|     163|\n",
            "|      populares|     119|\n",
            "|Pablo_Iglesias_|     109|\n",
            "|  AlbanoDante76|      95|\n",
            "|Front_Republica|      89|\n",
            "|           KRLS|      86|\n",
            "|sanchezcastejon|      79|\n",
            "|      JuntsXCat|      70|\n",
            "|       iescolar|      66|\n",
            "|         boye_g|      55|\n",
            "| AntonioMaestre|      52|\n",
            "|   pablocasado_|      48|\n",
            "|       ivanedlm|      46|\n",
            "| hermanntertsch|      42|\n",
            "|    CastigadorY|      40|\n",
            "|     eldiarioes|      37|\n",
            "+---------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inDegree = edgelist.groupBy(\"dst\")\\\n",
        "                 .agg(f.count('*'))\\\n",
        "                 .orderBy(\"count(1)\", ascending=0)\n",
        "inDegree = inDegree.withColumnRenamed(\"count(1)\", \"inDegree\")\n",
        "inDegree = inDegree.withColumnRenamed(\"dst\", \"screen_name\")\n",
        "\n",
        "inDegree.limit(20).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YTDC_FZpNTK"
      },
      "outputs": [],
      "source": [
        "output = inDegree.first()\n",
        "assert output.screen_name == \"vox_es\" and output.inDegree == 330, \"Incorrect output\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "u7ECqBpNpNTK"
      },
      "source": [
        "### **Part 4.2:** Graphframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870wrTRUpNTK"
      },
      "source": [
        "Este tipo de estructuras es muy común en muchos datasets y su análisis cada vez se ha vuelto más habitual. Para simplificar las operaciones y el análisis vamos a utilizar una librería específicamente diseñada para trabajar en redes en sistemas distribuidos: [**Graphframes**](https://graphframes.github.io/graphframes/docs/_site/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI-8P7DwpNTK"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "pyfiles = str(sc.getConf().get(u'spark.submit.pyFiles')).split(',')\n",
        "sys.path.extend(pyfiles)\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAUP_WKxpNTK"
      },
      "source": [
        "#### **Parte 4.2.1:** Crear un graph frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YGlZZakpNTL"
      },
      "source": [
        "Lo primero que vamos ha hacer es crear un objeto ```GraphFrame``` que contendrà toda la información de la red.\n",
        "\n",
        "En un paso previo ya hemos creado la *edge list* ahora vamos a crear una lista con los vértices. Crear una tabla ```vértices``` que contenga una única columna ```id``` con los nombre de usuario de todos los vértices. Recordad que hay vértices que puede que solo tengan aristas incidentes y otros que puede que no tengan (tenéis que utilizar la información de ambas columnas de la ```edgelist```). Recordad que la lista de vértices es un conjunto donde no puede haber repetición de identificadores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L-C7W7opNTL",
        "outputId": "bb1dfca6-0022-49a7-8ac6-0c972a08dbe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 5111 nodes on the network.\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"DROP TABLE IF EXISTS edgelist\")\n",
        "sqlContext.registerDataFrameAsTable(edgelist,\"edgelist\")\n",
        "\n",
        "vertices = sqlContext.sql(\"\"\" SELECT DISTINCT src as id\n",
        "                               FROM edgelist\n",
        "                               UNION\n",
        "                               SELECT DISTINCT dst as id\n",
        "                               FROM edgelist\n",
        "                               \"\"\")\n",
        "\n",
        "N = vertices.count()\n",
        "print(\"There are {0} nodes on the network.\".format(N))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipv1AwdIpNTL"
      },
      "outputs": [],
      "source": [
        "assert N == 5111, 'Incorrect output'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1gQYSZepNTL"
      },
      "source": [
        "Al igual que con las aristas, podéis asignar atributos a los vértices. Completad la tabla ```vertices``` haciendo un *inner join* por ```id``` con la tabla ```user_info``` guardada en el contexto ```hiveContext```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGjlCskWpNTL",
        "outputId": "f7f658ce-2f81-4826-ade9-17473d303eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+----+------+------------+---------+---------+\n",
            "|             id|lang|tweets|total_tweets|following|followers|\n",
            "+---------------+----+------+------------+---------+---------+\n",
            "|     00INVICTUS|  es|   797|       92552|    10658|    13304|\n",
            "|1000yUnaRazones|  es|   365|        3964|      286|       77|\n",
            "|        10Malva|  es|    88|       53042|     4926|     4920|\n",
            "|     11Esperena|  es|    69|      125587|    30231|    29012|\n",
            "|         14rnau|  ca|   595|       66999|      459|      216|\n",
            "|       1951195r|  es|   680|       67884|     1247|     2354|\n",
            "|    1960Anamari|  es|  1225|      182562|     4241|     5175|\n",
            "|     196193Jose|  es|   348|       96872|     1344|     1380|\n",
            "|       1963VIDA|  es|    35|          74|      123|       13|\n",
            "|     1denmadrid|  es|  2927|      760232|      305|     2509|\n",
            "|1ed06e2568ac4d8|  es|  1235|      593113|      130|     1766|\n",
            "|       1spandau|  es|    24|        2142|      117|       50|\n",
            "|        2011pau|  es|  1927|      121894|     1120|     3173|\n",
            "|       2017Grun|  es|  2256|       16745|      185|      352|\n",
            "|        20campy|  es|   360|        8851|      591|      102|\n",
            "|            20m|  es|   342|      287055|    51347|  1360853|\n",
            "|        233_rex|  es|   412|         741|      208|       21|\n",
            "|       2_juan23|  es|  1087|       27255|     6327|     5781|\n",
            "|      3000Dulce|  es|  1109|       90541|      793|     2031|\n",
            "|         324cat|  ca|   430|      312699|      323|   535287|\n",
            "+---------------+----+------+------------+---------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlContext.sql(\"DROP TABLE IF EXISTS vertices\")\n",
        "sqlContext.registerDataFrameAsTable(vertices,\"vertices\")\n",
        "\n",
        "vertices = sqlContext.sql(\"SELECT b.id, a.lang, a.tweets, a.total_tweets, a.following, a.followers FROM user_info a inner join vertices b on a.id = b.id\")\n",
        "\n",
        "vertices.limit(20).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoeINRJHpNTL"
      },
      "source": [
        "Una vez tenemos la edgelist y la lista de edges estamos en disposición de instanciar [un objecto ```GraphFrame```](https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html). Instanciad este objeto en la variable ```network```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0cwrMa0pNTL"
      },
      "outputs": [],
      "source": [
        "network = GraphFrame(vertices, edgelist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgrHHMyzpNTL"
      },
      "source": [
        "El objeto que acabais de crear tiene muchas atributos y métodos para el analisis de redes [(comprobad el API)](https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html). Se os pide que utilizéis el atributo ```inDegrees``` para, conjuntamente con la transformación ```orderBy```, mostrar la informació del in degree en orden descendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuM8rbJspNTM",
        "outputId": "d96ed46d-c828-4b2d-c580-ff14f31ce65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+--------+\n",
            "|             id|inDegree|\n",
            "+---------------+--------+\n",
            "|         vox_es|     330|\n",
            "|   ahorapodemos|     279|\n",
            "|           PSOE|     242|\n",
            "|   CiudadanosCs|     218|\n",
            "|  Santi_ABASCAL|     163|\n",
            "|      populares|     119|\n",
            "|Pablo_Iglesias_|     109|\n",
            "|  AlbanoDante76|      95|\n",
            "|Front_Republica|      89|\n",
            "|           KRLS|      86|\n",
            "|sanchezcastejon|      79|\n",
            "|      JuntsXCat|      70|\n",
            "|       iescolar|      66|\n",
            "|         boye_g|      55|\n",
            "| AntonioMaestre|      52|\n",
            "|   pablocasado_|      48|\n",
            "|       ivanedlm|      46|\n",
            "| hermanntertsch|      42|\n",
            "|    CastigadorY|      40|\n",
            "|     eldiarioes|      37|\n",
            "+---------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "network.inDegrees.orderBy('inDegree', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90zQRyo7pNTM"
      },
      "source": [
        "Haced lo mismo con el atributo ```outDegrees``` para, conjuntamente con la transformación ```orderBy```, mostrar la informació del out degree en orden descendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnjA6ha8pNTM",
        "outputId": "584a6ae0-1957-418c-c63b-e2d7fe9ec36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+---------+\n",
            "|             id|outDegree|\n",
            "+---------------+---------+\n",
            "|   rosavergar23|       11|\n",
            "|JulioAl18175505|       10|\n",
            "|      el_partal|       10|\n",
            "|    SSarelvis67|        9|\n",
            "|Teresaperezcep1|        8|\n",
            "|miguelgutiperez|        8|\n",
            "|       anap1958|        8|\n",
            "|       jasalo54|        7|\n",
            "|  Rafa_eltorete|        7|\n",
            "|     astroman78|        7|\n",
            "|   Socialista60|        7|\n",
            "|  yomismaconcha|        7|\n",
            "|    pacomarina6|        7|\n",
            "|        Fermirv|        7|\n",
            "|      MACUBERNA|        7|\n",
            "|        lyuva26|        7|\n",
            "| Perona10690463|        6|\n",
            "| PowerCaballero|        6|\n",
            "|       KilianCD|        6|\n",
            "|     carrasquem|        6|\n",
            "+---------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "network.outDegrees.orderBy('outDegree', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUlanOq1pNTM"
      },
      "source": [
        "#### **Parte 4.2.2:** Centralidad PageRank\n",
        "\n",
        "Hasta ahora hemos visto uno de los descriptores más básicos del análisis de redes, la centralidad de grado. Ahora vamos a aprovechar las funcionalidades de GraphFrames para estudiar [la centralidad del *PageRank*](https://en.wikipedia.org/wiki/PageRank), el algoritmo original que utilizaba Google para indexar la web.\n",
        "\n",
        "La idea detrás de este algoritmo es representar la reputación. Google pensaba que no solo es importante saber cuántos enlaces apuntan a una web, sino también su importancia. Para analizarlo crearon este algoritmo que básicamente queda formalmente representado por:\n",
        "$$\n",
        "\\mbox{PR}(p_i) = \\frac{1 - d}{N} + d \\sum_{p_j \\in M(p_i)}\\frac{\\mbox{PR}(p_j)}{L(p_j)}\n",
        "$$\n",
        "Donde $\\mbox{PR}(p_i)$ representa el PageRank de la página $p_i$, $d$ es un factor de amortiguación, $p_j$ es una página que enlaza $p_i$ y $L(p_j)$ es el numero total de enlaces salientes de la página $j$. Como podéis ver es un algoritmo recursivo, que se puede resolver de diferentes maneras.\n",
        "\n",
        "Afortunadamente, no tendréis que preocuparos por la implementación ya que la classe GraphFrames implementa un método ```pageRank``` para calcularlo. Cread una tabla ```page_rank``` con los resultados. Mostrad los resultados únicos con el ```id``` del nodo y su indice ```PageRank``` en orden descendente\n",
        "\n",
        "- ***Nota 1:*** Utilizando los parametros del metodo tenéis que fijar la probabilidad de reinicio a 0.15 y el numero máximo de iteraciones a 5\n",
        "- ***Nota 2:*** El tiempo de cómputo puede oscilar de 3 a 20 minutos dependiendo de la carga del servidor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Pl3shOppNTM"
      },
      "outputs": [],
      "source": [
        "page_rank = network.pageRank(resetProbability=0.15, maxIter=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miblLjK8pNTM",
        "outputId": "8c8791e5-a9c4-44c4-86be-a23f28b1cd22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+------------------+\n",
            "|             id|          pagerank|\n",
            "+---------------+------------------+\n",
            "|         vox_es|170.31097167053235|\n",
            "|Pablo_Iglesias_|120.80247412776232|\n",
            "|  Santi_ABASCAL|102.41446139478607|\n",
            "|   ahorapodemos| 100.2837043688577|\n",
            "| hermanntertsch|  95.4113047882803|\n",
            "|   CiudadanosCs|   93.245766612301|\n",
            "|           PSOE| 91.13458970143508|\n",
            "|  InesArrimadas|47.359707863261676|\n",
            "| RaiLopezCalvet| 47.34108812182462|\n",
            "|  Albert_Rivera| 47.31355661690909|\n",
            "|        ehbildu| 44.45332094685276|\n",
            "|      populares|41.608475611135525|\n",
            "|  AlbanoDante76| 34.74766697076182|\n",
            "|Front_Republica| 33.79746007120636|\n",
            "|        Jjsb441|31.125164754674714|\n",
            "|           KRLS| 30.45209286696975|\n",
            "|sanchezcastejon| 27.33777199805432|\n",
            "|      JuntsXCat| 26.74753185335384|\n",
            "|       iescolar|24.662781385904097|\n",
            "|         boye_g|21.129047610541164|\n",
            "+---------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#page_rank.inDegrees.orderBy('inDegree', ascending=False).show()\n",
        "\n",
        "page_rank.vertices.select(\"id\", \"pagerank\").distinct().orderBy('pagerank', ascending=False).show()\n",
        "\n",
        "#IMPORTANT-CHECK NO DUPLICATES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BuanDgUpNTN"
      },
      "source": [
        "¿Observáis alguna diferencia con los resultados de importancia del in degree?¿A que creéis que se debe?"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "bAQ0K0y_pNTN"
      },
      "source": [
        "Tenemos que ahorapodemos, PSOE y CiudadanosCs, que aparecían en el top4 se encuentran ahora en otras posiciones, quedando de los 3 solo ahorapodemos en este top. Esto puede deberse a que los retweets recibidos por estos usuarios proceden en su conjunto de usuarios menos relevantes de acuerdo con el algoritmo de pagerank. En general, se aprecian diferencias importantes entre la centralidad de grado y la de pagerank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_E0UrPMpNTN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false,
    "colab": {
      "name": "Glaria_lglaria_PEC_4___Analisis_avanzados_en_el_entorn_13-12-2020_19_38_48.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}